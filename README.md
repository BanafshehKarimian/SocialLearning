# SocialLearning
This research is about speeding the multi arm bandit and MDP problems with observations of other relevant or irrelevant agents. We propose a social
learning method for improving the performance of RL agents for the multi-armed bandit
setting. The social agent observes other agents’ decisions, while their rewards are private.
The agent uses a preference-based method, similar to the policy gradient learning method,
to fnd if there are any agents in the heterogeneous society worth learning from their
policies to improve its performance. The heterogeneity is the result of diversity in learning
algorithms, utility functions, and expertise. We then extend this method to MDP problem, generalize enough to address discrete and continous action and states. 
### What is social learning
We humans and even animals learn socially. As Kevin Laland says in his book, social learning is facilitating the learning through observation of, or interaction with, another individual or its product. The Social Learning Theory [[1]](#1) indicates that learning is a result of directly experiencing the skill or observing the others. This helps us to rapidly adapt to new environments, learni complex behaviour and we can say that without the ability to learn from social cues, the knowledge we now have was imposible to acquire through learning individualy[[1]](#1), [[2]](#2), [[3]](#3), [[4]](#4), [[5]](#5), [[6]](#6), [[7]](#7). <br />
Although highly effective, reinforcement learning as a standalone learning method is slow, sample inefficient, and has difficulty in adapting online to new situations. Thus using diﬀerent Social Learning methods in order to address this problems is an appropriate idea [[8]](#8). In order to do so, the following questions need to be answered:<br />
1:When to use social cues: When facing unknown task?, When our performance in a task is not good enough?, Always?<br />
2:How to detect which source of social cue is doing what we want or is informative for us?<br /><br />
3:How to use social cues?<br />
If we consider that an appropriate source of knowledge is present, Imitation Learning [[9]](#9) and Observational Learning (also known as Apprenticeship Learning [[10]](#10)) answer the question of "How to use the source". The feedback of human teacher is also used in some work [[11]](#11), [[12]](#12), [[13]](#13). The other two questions are underepresented in the literature and most of the work assume appropriate information is presented to the learner. In this research we are going to adress the first two questiones mentioned above.
### Problem Formulation
We first study improving multi-arm bandit problem using social learning and then extend our algorithm to MDP setting. For the first phase, we study the stochastic bandit problem that $A$ actions (arms) are available and $N$ other agents are present at the society who might or might not have the same goal as our agent and might have different level of expertise regarding their goals. Each action $a$ gives a reward with $E(q*(a))$ and each agent recieves a reward corresponding to the selected action. We assume that our agent has access to actions selected by other agents and wants to maximize its expected reward.<br />
For the second phase, we target an MDP problem $(S, A, P, R, \gamma)$, which $S$ and $A$ respectively correspound to state and action distribution, $P$ is the transition function, $R$ is the reward function and $\gamma$ is the discount factor. In this problem our agent has access to actions and states other agents are, and again wants to maximize its expected reward. 
### The Method
In orther to answer the second question, we can assume that selecting which agent or source of knowledge in the society is more apropriate for us is a multi-arm bandit problem itself. Plus, in order to answer when it is needed to use social cues, we can consider using our own internal learner as another arm. This could also makes the proposed algorithm more general and independent from the society. We improve the Gradient preference-based method to use it in our method. <br />
In the Gradient preference-based method, a numerical preference for each action (initially zero) is used to show that action’s relative preference over other actions. The actions are then selected according to Gibbs or Boltzmann distribution:<br />
$\Pr(A_t =a) = \frac{e^{H(a)}}{\sum_{all\hspace{1pt} action\hspace{1pt} b}(e^{H(b)})} =  \pi_t(A_t)$<br />
After performing the selected action and recieving reward, the preference to the selected action is increased and decreased for others if it's reward is greater than the mean reard and vice versa if the mean reward is greather than reward [[14]](#14). <br />
We use this method to find the appropriate agent in the environment, if any, or select action based on its internal learner if no appropriate agent is presented. If other agents in the society are selected, we select most frequently performed action of the sekected agent. Then after performing that action, we update our preference to the selected agent or our internal learner. This helps to find whether we are expert enough or are there any appropriate agent to follow. What we add to the preference based method is that, in addition to the selected agent, we update the preference to any agent that performed the same action or our internal learner if it suggested the same action. This helps to improve sample efficiency. Algorthm1, Algorithm2, and Algorithm3 show the method with details.<br />
![image](https://user-images.githubusercontent.com/19387425/188328828-d4156329-6d9b-4f75-809f-8cccea3204f6.png)
![image](https://user-images.githubusercontent.com/19387425/188328870-1a69c4d9-0860-446f-81b9-082868da1143.png)
![image](https://user-images.githubusercontent.com/19387425/188328880-4c1a09b9-5bc5-472f-ba76-9cb6c5d4e220.png)
### Results for multi-arm bandit
We test our method to analyze its performance. The rewards given from the environment are from the Gaussian distribution N(µ, 1) with µ randomly selected from {(k − 100)/10|k ∈ [0, 200]}. We did so to remove the effect of reward as a confounding variable. Other agents in the society are learner that uses Thompson sampling, Worst agent that always selects the worst action and Percent agent (P0, δP, Pmax) that the action with the most expected reward is selected with probability of P0 that is increased by δP each trial until reaching probability equal to Pmax.<br />
We first test is the proposed method robust to increase of population and the problem difculty. We compare the social learning to individual learning in four scenarios: 1) when the society is not populated ($N = 10$) and the problem is easy to solve ($k = 10$), 2) when the soicety is populated ($N = 100 random agent and one expert$) and the problem is hard ($k = 100 random agent and one expert$), 3) when the society is not populated ($N = 10 random agent and one expert$) and the problem is hard ($k = 100$), and 4) when the soicety is populated ($N = 100 random agent and one expert$) but the problem is easy to solve ($k = 10$). In the following figure we can see that when the society becomes populated ($N = 100$), it is hard for the social agent to detect the appropriate agent especially when the problem itself is easy to solve ($k = 10$), but the agent is able to reach the individual learner even in this worst case. We also can see that when the problem is hard ($k = 100$), the social learner always wins over the individual one even in a populated society. It is important to note that when the agents in a society form a cluster and select similar actions it is easier for the agent to update its preference with the change we have made to the preferenece update algorithm. Thus, we can say that when the number of the random agents are high is the most dificult case for our agent to detect. <br />
![image](https://user-images.githubusercontent.com/19387425/188329209-40c52e29-bc5b-4474-ab05-d2adfd1d5df1.png)<br />
We also test our agents ability to detect the quality of other agents with respect to its goal. we can see that it can detect the others quality throughly and when the situation changes, the agent is able to recognize that and update the preference accordingly. For example an agent with slower learning but higher threshold (the blue agent) getting better than an agent with faster learning but a limit thereshol (the cyan agent), the agent increases the former agents reference and decreases the laters preference as well.<br />
![image](https://user-images.githubusercontent.com/19387425/188329758-c590fbde-9fe9-42ac-9b02-4d82158c6fe2.png)<br />
### Extend to MDP
In this section we introduce how we extend our method to MDP. It is important to note that our method can work and extend any individual Reinforcement Learning method to social learning and one only needs to replace the action selection part of the Reinforcement Learning method they have in mind with the social action selection core of our method and call the social update after performing each action. <br />
Algorithm 1 shows the social loop that is followed at each step. The first thing the agent needs to do is to update its observations. After that the agent needs to select an agent in the society including itself to follow based on its preference to them. If the agent selects itself, it needs to call the action selector method of its individual learning method. Otherwise if any other agent is selected, for continuous states our agent needs to find the closest state that it saw the selected agent perform actions. If states are discrete, the agent checks if it observed the other agent in the exact same state as it is now in. If the state does not exist in the discrete case and if its distance to the nearest state in the continues state case is more than a threshold, the agent needs to call its individual learning method to select an action. This method works for both discrete and continious action with "get_action" function returning the most frequent action and the average action of the selected agent in discerete and continous case respectively.<br />
![image](https://user-images.githubusercontent.com/19387425/191490726-2b546e80-c377-4e03-9049-44ed2545c2c9.png)<br />
Algorithm2 shows the observation update procedure. After observing actions and states for each agent in the society, we check if the observed state is not in the state memory of that agent, or not close to seen states in continoues case. If true, then the state and action are added as new state and action. Otherwise, if the state, or a close state in continoues case, exists, the action memory is updated. The action memory update calculates the frequency of actions for disceret actions and action average for continuous ones.<br />
![image](https://user-images.githubusercontent.com/19387425/191525447-1d1e59af-3c74-4235-9cc5-081fb0594938.png)<br />
### First Results
We test our method on BRAX, which is a differentiable physics engine that simulates environments made up of rigid bodies, joints, and actuators. The advantage of BRAX is that we can run it on google COLAB that gives us access to GPU and TPU. We first focus on the ANT environment, which has continuous state and action spaces with a dimension of 87 and 8. The reward is calculated from the following formulas.<br />
Reward =  forward_reward - ctrl_cost - contact_cost + 1<br />
forward_reward = (x_after - x_before) / config.dt<br />
ctrl_cost = .5 * sum(square(action))<br />
contact_cost = (0.5 * 1e-3 *sum(square(clip(contact.vel, -1, 1))))<br />
We add social learning to DDPG and test it in a problem with continuous state and action spaces. The following figure shows the Cumulative reward of each 1000 trials learning individually using DDPG and social+DDPG learning in two societies: a) 1 agent that is 50% expert and 50% random, 1 totally random agent and b)  agent that is 50% expert and 50% random, 30 totally random agents. We can see that the social + DDPG algorithm converges faster than the individual DDPG algorithm.
![image](https://user-images.githubusercontent.com/19387425/191530459-16a49990-2000-4295-843d-a843ac30c3c7.png)
![image](https://user-images.githubusercontent.com/19387425/191530526-3ce35362-6afa-43a8-b37e-5aad02aa19b9.png)

### Reference
<a name="1">[1]</a> Albert Bandura and Richard H Walters. Social learning theory, volume 1. Englewood cliﬀs Prentice Hall, 1977.<br />
<a name="2">[2]</a> Joseph Henrich and Richard McElreath. The evolution of cultural evolution. Evolutionary Anthropology: Issues, News, and Reviews: Issues, News, and Reviews, 12(3):123–135, 2003.<br />
<a name="3">[3]</a>Kevin N Laland. Social learning strategies. Animal Learning & Behavior, 32(1):4–14, 2004.<br />
<a name="4">[4]</a>Yuval Noah Harari. Sapiens: A Brief History of Humankind. McClelland Stewart, 1st edition, 2014.<br />
<a name="5">[5]</a>Joseph Henrich. The Secret of Our Success: How Culture Is Driving Human Evolution, Domesticating Our Species, and Making Us Smarter. Princeton University Press, 1st edition, 2015.<br />
<a name="6">[6]</a>Kevin N. Laland. Darwin’s Unfnished Symphony: How Culture Made the Human Mind. Princeton University Press, 1st edition, 2017.<br />
<a name="7">[7]</a>Max Kleiman-Weiner. Computational foundations of human social intelligence. PhD thesis, Massachusetts Institute of Technology, 2018.<br />
<a name="8">[8]</a>Kamal K Ndousse, Douglas Eck, Sergey Levine, and Natasha Jaques. Emergent social learning via multi-agent reinforcement learning. In International Conference on Machine Learning, pages 7991–8004. PMLR, 2021.<br />
<a name="9">[9]</a>Yan Duan, Marcin Andrychowicz, Bradly C Stadie, Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. In NIPS, 2017.<br />
<a name="10">[10]</a>Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-frst international conference on Machine learning, page 1, 2004.<br />
<a name="11">[11]</a>W Bradley Knox and Peter Stone. Interactively shaping agents via human reinforcement: The tamer framework. In Proceedings of the ffth international conference on Knowledge capture, pages 9–16, 2009.<br />
<a name="12">[12]</a>Paul Christiano, Jan Leike, Tom B Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. arXiv preprint arXiv:1706.03741, 2017.<br />
<a name="13">[13]</a>Dorsa Sadigh, Anca D. Dragan, S. Shankar Sastry, and Sanjit A. Seshia. Active preferencebased learning of reward functions. In Robotics: Science and Systems, 2017.<br />
<a name="14">[14]</a>Richard S Sutton and Andrew G Barto. Reinforcement learning:An introduction. the MIT Press, 2nd edition, 2018.
